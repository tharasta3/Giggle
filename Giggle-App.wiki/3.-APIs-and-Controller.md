The engine for our app interacts with several third-party APIs to enable functionalities such as image tagging, sentiment analysis, and meme generation. The app’s front-end (built using SwiftUI) communicates with the engine (hosted on the backend using Django) via HTTP requests facilitated by Alamofire and URLSession. The backend acts as a controller that processes requests, handles data, and interfaces with the third-party APIs. All of the memes are stored with SwiftData, locally on the user's device.


## Third Party SDKs

### Alamofire
We use the Alamofire package to make requests from the front end to the back end; this includes sending memes in image format and text. Alamofire documentation and download can be found [here](https://github.com/Alamofire/Alamofire).

Link to use in code:
https://github.com/rgperry/Giggle/blob/945ed7b3ace1529fa470df576a1f2ef760626e66/Giggle/Giggle/Datastore.swift#L282C9-L288C19


### Giffy
We use Giffy to display GIFs after a user clicks on a meme to view it. The package can be installed and added to a project as a dependency [here](https://github.com/tfmart/giffy).

Link to use in code: https://github.com/rgperry/Giggle/blob/945ed7b3ace1529fa470df576a1f2ef760626e66/Giggle/Giggle/MemeInfoView.swift#L37C17-L37C48

<br>

## Back-end:

### 1. Get Image Info

Purpose: Automatically generates a list of tags, as well as a text description of an image with OpenAI's ChatGPT 4o-mini model.
- {x} one to two word tags are returned, based on the sentiment, people, content, text, etc. in the image
- a description of {y} length is returned that includes the text in the image, as well as a longer description of the emotion and content that can't be solely be captured by the tags

[link to endpoint](https://github.com/rgperry/Giggle/blob/main/Giggle_backend/app/views.py#L77-L123)

[helper that makes the gpt call](https://github.com/rgperry/Giggle/blob/e65516745a204f650d31be8362c3ad91a2ad72b4/Giggle_backend/app/utils.py#L113-L198)

Endpoint: POST /imageInfo/

**Request Parameters**

Key | Location | Type | Description | Default Value
-- | -- | -- | -- | --
numTags | Query Parameter | Int | Recommended number of tags to generate: x | 10
contentLength | Query Parameter | Int | Length of text description to return: y | 200
image | multiPart form data | File | file path of the image to analyze

**Response Codes**

Code | Description
-- | --
200 OK | Success
400 Bad Request | Invalid parameters (missing image)

**Returns**

Key | Location | Type | Description
-- | -- | -- | --
tags | JSON | [String] | Tags for the image
content | JSON | String | Content description of the image

**Example** <br>
<img src="https://github.com/user-attachments/assets/50abcee8-e417-4d2f-aba1-f06f40bcbc70" alt="description" width="300" height="200"> <br>
curl --request POST \
  --url 'https://18.223.212.43/getInfo/?contentLength=200&numTags=10' \
  --header 'Content-Type: multipart/form-data' \
  --header 'User-Agent: insomnia/10.1.1' \
  --form image=@/Users/user/Desktop/memes/pngs/fuji_meme.png

```json
{
  "tags": [
    "sadness",
    "happiness",
    "outdoors",
    "mount fuji",
    "nature",
    "emotion",
    "snowfall",
    "memes",
    "journey",
    "tranquility"
  ],
  "content": "The meme depicts a simple emotional journey. At the top, a sad man wearing a black beanie and hoodie is labeled \"Man sad.\" He steps outside, as shown in the next frame with the text \"Man goes outside.\" In the third frame, he sees Mount Fuji covered with snow, accompanied by the text \"Man sees Snow back on Mount Fuji.\" Finally, his expression changes to happiness, and the caption reads, \"Man happy.\" The characters, drawn in a popular internet meme style, convey a progression from melancholy to joy, highlighting the restorative power of nature."
}
```

API calls a helper function which communicates with ChatGPT 4o-mini model through the vision feature as follows:

```python
from openai import OpenAI

client = OpenAI()

response = client.chat.completions.create(
  model="gpt-4o-mini",
  messages=[
    {
      "role": "user",
      "content": [
        {"type": "text", "text": f" ... {numTags/contentLength} ...",
        {
          "type": "image_url",
          "image_url": {
            "url": processed_image,
          },
        },
      ],
    }
  ],
  max_tokens=300,
)

print(response.choices[0])
```

After processing the 'image' into a url, we use it for the image_url of the call.

We use the text in the content with 'numTags' and a prompt to request tags generated for the image we obtained in the endpoint to generate tags. 

We also use the text in the content with 'contentLength' and a prompt to request content of the image we obtained in the endpoint to extract content.

### 2. Message Sentiment Analyzer (With OpenAI ChatGPT 4o-mini)
Purpose: Select relevant tags for a conversation message based on the content, but primarily the sentiment/emotion of the message.

Endpoint: GET /getSentiment/

[link to endpoint](https://github.com/rgperry/Giggle/blob/main/Giggle_backend/app/views.py#L57-L75)

[helper that makes the gpt call](https://github.com/rgperry/Giggle/blob/e65516745a204f650d31be8362c3ad91a2ad72b4/Giggle_backend/app/utils.py#L74-L110)

**Request Parameters**
Key | Location | Type | Description
-- | -- | -- | -- 
message | Query Parameter | String | Conversation's message to be analyzed by sentiment.
tags | Query Parameter | String | List of tags in the database.

**Response Codes**
Code | Description
-- | --
200 OK | Success
400 Bad Request | Invalid parameters (empty message or tags)

**Returns**

Key | Location | Type | Description
-- | -- | -- | --
relevantTags | JSON | String | Relevant tags for an image, separated by spaces.

**Example**

curl --request GET \
  --url 'https://18.223.212.43/getSentiment/?message=I%20can'\''t%20wait%20to%20go%20to%20the%20football%20game%20tomorrow!&tags=%22happy%20sad%20excited%22'

```json
{
  "relevantTags": "happy excited"
}
```

API calls a helper function which communicates with ChatGPT 4o-mini model through the text-generation feature as follows:

```python
from openai import OpenAI
client = OpenAI()

completion = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {
            "role": "user",
            "content": f"... {message} ... {tags} ..."
        }
    ]
)

print(completion.choices[0].message)
```

We use the content for the system to make the AI a sentiment analyzing assistant and give the 'message' and 'tags' as a part of a prompt, which asks for sentiment in the form of tags based on our current tags, in the content for the user. 

### 3. Image Generation API (OpenAI DALL·E 3 API)
Purpose: Generates a new meme image based on a text prompt submitted by the user.

Endpoint: GET /generateMeme/

[link to endpoint](https://github.com/rgperry/Giggle/blob/main/Giggle_backend/app/views.py#L12-L31)

[helper that makes the gpt call](https://github.com/rgperry/Giggle/blob/e65516745a204f650d31be8362c3ad91a2ad72b4/Giggle_backend/app/utils.py#L11-L42)

**Request Parameters**

Key | Location | Type | Description
-- | -- | -- | -- 
description | Query Parameter | String | Description of the meme to be generated

**Response Codes**
Code | Description
-- | --
200 OK | Success
400 Bad Request | Invalid parameters (no description or image)

**Returns**

Key | Location | Type | Description
-- | -- | -- | --
image | JSON | String | Base64 encoded image

**Example**

curl --request POST \
  --url 'https://18.223.212.43/getSentiment/?description=%22cat%20wearing%20a%20hat%22'

```json
{
  "image": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAnYA..."
}
```

How it works:
- The front-end sends a text query provided by the user to the backend.
- The backend communicates with the DALL·E 3 API to generate an image matching the query.
- The backend stores the generated image in the database and returns the URL of the image to the front-end for display.

Front-End Communication:
- A GET request is sent from the front-end to the backend with the user's text prompt.
- The backend processes the request, sends it to DALL·E 3 API, and returns the generated image back to the front-end, which displays it.


API calls a helper function which communicates with DALL·E 3 model through the image-generation feature as follows:

```python
from openai import OpenAI
client = OpenAI()

response = client.images.generate(
  model="dall-e-3",
  prompt=description,
  size="1024x1024",
  quality="standard",
  n=1,
)

image_url = response.data[0].url
```

We use the 'description' obtained from the endpoint as our prompt.
